{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rTy517ZUXJJ3vbmSUvjK-eIdFiv6Z-Qk","authorship_tag":"ABX9TyOZZEpW2iijEpe4cYbTiVHv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dwd_dOse7hCP","executionInfo":{"status":"ok","timestamp":1718875244855,"user_tz":-330,"elapsed":21622,"user":{"displayName":"Merugu Sriram","userId":"00694942759061811370"}},"outputId":"615270e4-de83-406c-927a-647240d6afbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","6/6 [==============================] - 5s 10ms/step - loss: 0.6924 - accuracy: 0.6211\n","Epoch 2/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.6902 - accuracy: 0.6289\n","Epoch 3/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.6289\n","Epoch 4/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.6817 - accuracy: 0.6777\n","Epoch 5/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.6738 - accuracy: 0.7637\n","Epoch 6/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.8535\n","Epoch 7/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.6428 - accuracy: 0.9043\n","Epoch 8/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.6175 - accuracy: 0.9355\n","Epoch 9/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.5836 - accuracy: 0.9375\n","Epoch 10/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.5424 - accuracy: 0.9453\n","Epoch 11/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.4941 - accuracy: 0.9453\n","Epoch 12/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.4402 - accuracy: 0.9453\n","Epoch 13/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.3887 - accuracy: 0.9355\n","Epoch 14/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.3404 - accuracy: 0.9395\n","Epoch 15/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.2996 - accuracy: 0.9395\n","Epoch 16/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.2612 - accuracy: 0.9473\n","Epoch 17/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.2263 - accuracy: 0.9453\n","Epoch 18/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.2087 - accuracy: 0.9551\n","Epoch 19/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1889 - accuracy: 0.9551\n","Epoch 20/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9551\n","Epoch 21/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9629\n","Epoch 22/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9609\n","Epoch 23/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.9629\n","Epoch 24/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1304 - accuracy: 0.9629\n","Epoch 25/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9668\n","Epoch 26/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1161 - accuracy: 0.9707\n","Epoch 27/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1077 - accuracy: 0.9707\n","Epoch 28/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1096 - accuracy: 0.9727\n","Epoch 29/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.9746\n","Epoch 30/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9766\n","Epoch 31/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9785\n","Epoch 32/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0920 - accuracy: 0.9785\n","Epoch 33/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9785\n","Epoch 34/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9824\n","Epoch 35/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0772 - accuracy: 0.9824\n","Epoch 36/150\n","6/6 [==============================] - 0s 7ms/step - loss: 0.0813 - accuracy: 0.9785\n","Epoch 37/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0749 - accuracy: 0.9844\n","Epoch 38/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.9805\n","Epoch 39/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9824\n","Epoch 40/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.9805\n","Epoch 41/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0699 - accuracy: 0.9844\n","Epoch 42/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0749 - accuracy: 0.9824\n","Epoch 43/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9863\n","Epoch 44/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9863\n","Epoch 45/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9824\n","Epoch 46/150\n","6/6 [==============================] - 0s 6ms/step - loss: 0.0708 - accuracy: 0.9844\n","Epoch 47/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.9863\n","Epoch 48/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0647 - accuracy: 0.9844\n","Epoch 49/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0670 - accuracy: 0.9844\n","Epoch 50/150\n","6/6 [==============================] - 0s 6ms/step - loss: 0.0671 - accuracy: 0.9863\n","Epoch 51/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0649 - accuracy: 0.9844\n","Epoch 52/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0596 - accuracy: 0.9863\n","Epoch 53/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9863\n","Epoch 54/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0603 - accuracy: 0.9863\n","Epoch 55/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 0.9844\n","Epoch 56/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 0.9863\n","Epoch 57/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0599 - accuracy: 0.9844\n","Epoch 58/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0635 - accuracy: 0.9844\n","Epoch 59/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9844\n","Epoch 60/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9863\n","Epoch 61/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9844\n","Epoch 62/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0591 - accuracy: 0.9863\n","Epoch 63/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0585 - accuracy: 0.9844\n","Epoch 64/150\n","6/6 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9863\n","Epoch 65/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9844\n","Epoch 66/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.9844\n","Epoch 67/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9844\n","Epoch 68/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9863\n","Epoch 69/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9863\n","Epoch 70/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.9844\n","Epoch 71/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9863\n","Epoch 72/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0542 - accuracy: 0.9844\n","Epoch 73/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0558 - accuracy: 0.9844\n","Epoch 74/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0488 - accuracy: 0.9883\n","Epoch 75/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0533 - accuracy: 0.9844\n","Epoch 76/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9863\n","Epoch 77/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0481 - accuracy: 0.9863\n","Epoch 78/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0442 - accuracy: 0.9883\n","Epoch 79/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0520 - accuracy: 0.9863\n","Epoch 80/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9863\n","Epoch 81/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9863\n","Epoch 82/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9863\n","Epoch 83/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 0.9883\n","Epoch 84/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9883\n","Epoch 85/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9883\n","Epoch 86/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - accuracy: 0.9883\n","Epoch 87/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.9883\n","Epoch 88/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9883\n","Epoch 89/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0509 - accuracy: 0.9863\n","Epoch 90/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0500 - accuracy: 0.9863\n","Epoch 91/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9883\n","Epoch 92/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.9844\n","Epoch 93/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0488 - accuracy: 0.9883\n","Epoch 94/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.9863\n","Epoch 95/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0456 - accuracy: 0.9902\n","Epoch 96/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0483 - accuracy: 0.9883\n","Epoch 97/150\n","6/6 [==============================] - 0s 6ms/step - loss: 0.0487 - accuracy: 0.9863\n","Epoch 98/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.9902\n","Epoch 99/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.9844\n","Epoch 100/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0460 - accuracy: 0.9883\n","Epoch 101/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0469 - accuracy: 0.9863\n","Epoch 102/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0504 - accuracy: 0.9883\n","Epoch 103/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9902\n","Epoch 104/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0488 - accuracy: 0.9883\n","Epoch 105/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0449 - accuracy: 0.9863\n","Epoch 106/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0481 - accuracy: 0.9902\n","Epoch 107/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9902\n","Epoch 108/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0447 - accuracy: 0.9883\n","Epoch 109/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0517 - accuracy: 0.9883\n","Epoch 110/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.9883\n","Epoch 111/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0434 - accuracy: 0.9902\n","Epoch 112/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0429 - accuracy: 0.9883\n","Epoch 113/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0432 - accuracy: 0.9883\n","Epoch 114/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0426 - accuracy: 0.9922\n","Epoch 115/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.9883\n","Epoch 116/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0405 - accuracy: 0.9902\n","Epoch 117/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0405 - accuracy: 0.9902\n","Epoch 118/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.9922\n","Epoch 119/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.9863\n","Epoch 120/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9902\n","Epoch 121/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0446 - accuracy: 0.9902\n","Epoch 122/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.9922\n","Epoch 123/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.9922\n","Epoch 124/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0429 - accuracy: 0.9922\n","Epoch 125/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0411 - accuracy: 0.9902\n","Epoch 126/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.9922\n","Epoch 127/150\n","6/6 [==============================] - 0s 6ms/step - loss: 0.0397 - accuracy: 0.9922\n","Epoch 128/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9922\n","Epoch 129/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9922\n","Epoch 130/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.9902\n","Epoch 131/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9902\n","Epoch 132/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0370 - accuracy: 0.9922\n","Epoch 133/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9922\n","Epoch 134/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.9922\n","Epoch 135/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0405 - accuracy: 0.9902\n","Epoch 136/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.9902\n","Epoch 137/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9922\n","Epoch 138/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 0.9922\n","Epoch 139/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0416 - accuracy: 0.9883\n","Epoch 140/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9922\n","Epoch 141/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9902\n","Epoch 142/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9902\n","Epoch 143/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.9883\n","Epoch 144/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0416 - accuracy: 0.9922\n","Epoch 145/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.9922\n","Epoch 146/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 0.9902\n","Epoch 147/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9922\n","Epoch 148/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9922\n","Epoch 149/150\n","6/6 [==============================] - 0s 4ms/step - loss: 0.0446 - accuracy: 0.9902\n","Epoch 150/150\n","6/6 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9902\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7e1402517cd0>"]},"metadata":{},"execution_count":2}],"source":["\n","#!/usr/bin/env python\n","# coding: utf-8\n","\n","# In[1]:\n","\n","\n","# Importing libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# In[2]:\n","\n","\n","data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/MLNN/Week-8/breast cancer_week 8 and 9.csv')\n","del data['Unnamed: 32']\n","\n","\n","# In[3]:\n","\n","\n","X = data.iloc[:, 2:].values\n","y = data.iloc[:, 1].values\n","\n","\n","# In[4]:\n","\n","\n","# Encoding categorical data\n","from sklearn.preprocessing import LabelEncoder\n","labelencoder_X_1 = LabelEncoder()\n","y = labelencoder_X_1.fit_transform(y)\n","\n","\n","# In[5]:\n","\n","\n","# Splitting the dataset into the Training set and Test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n","\n","\n","# In[6]:\n","\n","\n","#Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","\n","# In[7]:\n","\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","\n","\n","# In[8]:\n","\n","\n","# Initialising the ANN\n","classifier = Sequential()\n","\n","\n","# In[12]:\n","\n","\n","# Adding the input layer and the first hidden layer\n","classifier.add(Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\"))\n","# Adding dropout to prevent overfitting\n","classifier.add(Dropout(rate=0.1))\n","\n","\n","# In[16]:\n","\n","\n","# Adding the second hidden layer\n","classifier.add(Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\"))\n","# Adding dropout to prevent overfitting\n","classifier.add(Dropout(rate=0.1))\n","\n","\n","# In[19]:\n","\n","\n","# Adding the output layer\n","classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n","\n","\n","# In[20]:\n","\n","\n","#Compiling the ANN\n","classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","\n","# In[22]:\n","\n","\n","# Fitting the ANN to the Training set\n","classifier.fit(X_train, y_train, batch_size=100, epochs=150)\n","\n","\n","# In[ ]:\n","\n"]}]}