{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPEOMhiawL4WoF1ADX1tXOw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":966},"id":"DEAjVBGaCEot","executionInfo":{"status":"error","timestamp":1718876795482,"user_tz":-330,"elapsed":115165,"user":{"displayName":"Merugu Sriram","userId":"00694942759061811370"}},"outputId":"5f131de2-9495-4d4f-f1f3-2e0bf7fdee68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting imbalanced-learn==0.6.0\n","  Downloading imbalanced_learn-0.6.0-py3-none-any.whl (162 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.6.0) (1.25.2)\n","Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.6.0) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.6.0) (1.2.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.6.0) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->imbalanced-learn==0.6.0) (3.5.0)\n","Installing collected packages: imbalanced-learn\n","  Attempting uninstall: imbalanced-learn\n","    Found existing installation: imbalanced-learn 0.10.1\n","    Uninstalling imbalanced-learn-0.10.1:\n","      Successfully uninstalled imbalanced-learn-0.10.1\n","Successfully installed imbalanced-learn-0.6.0\n","Collecting scikit-learn==0.22.1\n","  Downloading scikit-learn-0.22.1.tar.gz (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.22.1) (1.25.2)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.22.1) (1.11.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.22.1) (1.4.2)\n","Building wheels for collected packages: scikit-learn\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for scikit-learn\n","Failed to build scikit-learn\n","\u001b[31mERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]},{"output_type":"error","ename":"TypeError","evalue":"duplicate base class Sequence","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e1119a1b51bf>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mover_sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m in keras.\"\"\"\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbalanced_batch_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/keras/_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBalancedBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mParentClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \"\"\"Create balanced batches when training a keras model.\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: duplicate base class Sequence"]}],"source":["get_ipython().system('pip install imbalanced-learn==0.6.0')\n","get_ipython().system('pip install scikit-learn==0.22.1')\n","\n","\n","# In[4]:\n","\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import time\n","import warnings\n","import numpy as np\n","from nltk.corpus import stopwords\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.preprocessing import normalize\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.manifold import TSNE\n","import seaborn as sns\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import SGDClassifier\n","from imblearn.over_sampling import SMOTE\n","from collections import Counter\n","from scipy.sparse import hstack\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.svm import SVC\n","from sklearn.model_selection import StratifiedKFold\n","from collections import Counter, defaultdict\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","import math\n","from sklearn.metrics import normalized_mutual_info_score\n","from sklearn.ensemble import RandomForestClassifier\n","warnings.filterwarnings(\"ignore\")\n","import six\n","import sys\n","sys.modules['sklearn.externals.six'] = six\n","from mlxtend.classifier import StackingClassifier\n","from sklearn import model_selection\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","# In[5]:\n","\n","\n","get_ipython().system('gdown --id 1RmX5_q6D7rzoXD7nPUM_s8rKEf1KVMDi #training_text.zip download')\n","get_ipython().system('gdown --id 1bSQrw5WmDqqI8hBcr8Pflzatx4xCT0Ex #training_variants.zip download')\n","\n","\n","# In[7]:\n","\n","\n","data = pd.read_csv('C:/Users/saile/Desktop/training_variants')\n","print('Number of data points : ', data.shape[0])\n","print('Number of features : ', data.shape[1])\n","print('Features : ', data.columns.values)\n","data.head()\n","\n","\n","# In[10]:\n","\n","\n","data_text =pd.read_csv('C:/Users/saile/Desktop/training_text',sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\n","print('Number of data points : ', data_text.shape[0])\n","print('Number of features : ', data_text.shape[1])\n","print('Features : ', data_text.columns.values)\n","data_text.head()\n","\n","\n","# In[11]:\n","\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","\n","# In[12]:\n","\n","\n","# loading stop words from nltk library\n","stop_words = set(stopwords.words('english'))\n","def nlp_preprocessing(total_text, index, column):\n","    if type(total_text) is not int:\n","        string = \"\"\n","        # replace every special char with space\n","        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n","        # replace multiple spaces with single space\n","        total_text = re.sub('\\s+',' ', total_text)\n","        # converting all the chars into lower-case.\n","        total_text = total_text.lower()\n","\n","        for word in total_text.split():\n","        # if the word is a not a stop word then retain that word from the data\n","            if not word in stop_words:\n","                string += word + \" \"\n","\n","        data_text[column][index] = string\n","\n","\n","# In[16]:\n","\n","\n","#text processing stage.\n","start_time = time.perf_counter()\n","for index, row in data_text.iterrows():\n","    if type(row['TEXT']) is str:\n","        nlp_preprocessing(row['TEXT'], index, 'TEXT')\n","    else:\n","        print(\"there is no text description for id:\",index)\n","print('Time took for preprocessing the text :',time.perf_counter() - start_time, \"seconds\")\n","\n","\n","# In[17]:\n","\n","\n","#merging both gene_variations and text data based on ID\n","result = pd.merge(data, data_text,on='ID', how='left')\n","result.head()\n","\n","\n","# In[18]:\n","\n","\n","result[result.isnull().any(axis=1)]\n","\n","\n","# In[19]:\n","\n","\n","result.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+result['Variation']\n","\n","\n","# In[20]:\n","\n","\n","result[result['ID']==1109]\n","\n","\n","# In[21]:\n","\n","\n","y_true = result['Class'].values\n","result.Gene      = result.Gene.str.replace('\\s+', '_')\n","result.Variation = result.Variation.str.replace('\\s+', '_')\n","\n","\n","# In[24]:\n","\n","\n","from sklearn.model_selection import train_test_split\n","# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\n","X_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\n","# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\n","train_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n","\n","\n","# In[25]:\n","\n","\n","print('Number of data points in train data:', train_df.shape[0])\n","print('Number of data points in test data:', test_df.shape[0])\n","print('Number of data points in cross validation data:', cv_df.shape[0])\n","\n","\n","# In[34]:\n","\n","\n","train_class_distribution = train_df['Class'].value_counts().sort_index()\n","test_class_distribution = test_df['Class'].value_counts().sort_index()\n","cv_class_distribution = cv_df['Class'].value_counts().sort_index()\n","my_colors = 'rgbkymc'\n","train_class_distribution.plot(kind='bar')\n","plt.xlabel('Class')\n","plt.ylabel('Data points per Class')\n","plt.title('Distribution of yi in train data')\n","plt.grid()\n","plt.show()\n","\n","  # ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n","  # -(train_class_distribution.values): the minus sign will give us in decreasing order\n","sorted_yi = np.argsort(-train_class_distribution.values)\n","for i in sorted_yi:\n","        print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/train_df.shape[0]*100), 3), '%)')\n","\n","\n","print('-'*80)\n","my_colors = 'rgbkymc'\n","test_class_distribution.plot(kind='bar')\n","plt.xlabel('Class')\n","plt.ylabel('Data points per Class')\n","plt.title('Distribution of yi in test data')\n","plt.grid()\n","plt.show()\n","\n","# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n","# -(train_class_distribution.values): the minus sign will give us in decreasing order\n","sorted_yi = np.argsort(-test_class_distribution.values)\n","for i in sorted_yi:\n","    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]/test_df.shape[0]*100), 3), '%)')\n","\n","print('-'*80)\n","my_colors = 'rgbkymc'\n","cv_class_distribution.plot(kind='bar')\n","plt.xlabel('Class')\n","plt.ylabel('Data points per Class')\n","plt.title('Distribution of yi in cross validation data')\n","plt.grid()\n","plt.show()\n","\n","# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n","# -(train_class_distribution.values): the minus sign will give us in decreasing order\n","sorted_yi = np.argsort(-train_class_distribution.values)\n","for i in sorted_yi:\n","        print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]/cv_df.shape[0]*100), 3), '%)')\n","\n","\n","# In[43]:\n","\n","\n","def predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n","    clf.fit(train_x, train_y)\n","    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","    sig_clf.fit(train_x, train_y)\n","    pred_y = sig_clf.predict(test_x)\n","\n","    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n","    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n","    # calculating the number of data points that are misclassified\n","    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n","    plot_confusion_matrix(test_y, pred_y)\n","\n","\n","# In[55]:\n","\n","\n","def get_gv_fea_dict(alpha, feature, df):\n","    value_count = train_df[feature].value_counts()\n","\n","    # gv_dict : Gene Variation Dict, which contains the probability array for each gene/variation\n","    gv_dict = dict()\n","\n","    # denominator will contain the number of time that particular feature occured in whole data\n","    for i, denominator in value_count.items():\n","        # vec will contain (p(yi==1/Gi) probability of gene/variation belongs to perticular class\n","        # vec is 9 diamensional vector\n","        vec = []\n","        for k in range(1,10):\n","            # print(train_df.loc[(train_df['Class']==1) & (train_df['Gene']=='BRCA1')])\n","            #         ID   Gene             Variation  Class\n","            # 2470  2470  BRCA1                S1715C      1\n","            # 2486  2486  BRCA1                S1841R      1\n","            # 2614  2614  BRCA1                   M1R      1\n","            # 2432  2432  BRCA1                L1657P      1\n","            # 2567  2567  BRCA1                T1685A      1\n","            # 2583  2583  BRCA1                E1660G      1\n","            # 2634  2634  BRCA1                W1718L      1\n","            # cls_cnt.shape[0] will return the number of rows\n","\n","            cls_cnt = train_df.loc[(train_df['Class']==k) & (train_df[feature]==i)]\n","\n","            # cls_cnt.shape[0](numerator) will contain the number of time that particular feature occured in whole data\n","            vec.append((cls_cnt.shape[0] + alpha*10)/ (denominator + 90*alpha))\n","\n","        # we are adding the gene/variation to the dict as key and vec as value\n","        gv_dict[i]=vec\n","    return gv_dict\n","\n","# Get Gene variation feature\n","def get_gv_feature(alpha, feature, df):\n","    # print(gv_dict)\n","    #     {'BRCA1': [0.20075757575757575, 0.03787878787878788, 0.068181818181818177, 0.13636363636363635, 0.25, 0.19318181818181818, 0.03787878787878788, 0.03787878787878788, 0.03787878787878788],\n","    #      'TP53': [0.32142857142857145, 0.061224489795918366, 0.061224489795918366, 0.27040816326530615, 0.061224489795918366, 0.066326530612244902, 0.051020408163265307, 0.051020408163265307, 0.056122448979591837],\n","    #      'EGFR': [0.056818181818181816, 0.21590909090909091, 0.0625, 0.068181818181818177, 0.068181818181818177, 0.0625, 0.34659090909090912, 0.0625, 0.056818181818181816],\n","    #      'BRCA2': [0.13333333333333333, 0.060606060606060608, 0.060606060606060608, 0.078787878787878782, 0.1393939393939394, 0.34545454545454546, 0.060606060606060608, 0.060606060606060608, 0.060606060606060608],\n","    #      'PTEN': [0.069182389937106917, 0.062893081761006289, 0.069182389937106917, 0.46540880503144655, 0.075471698113207544, 0.062893081761006289, 0.069182389937106917, 0.062893081761006289, 0.062893081761006289],\n","    #      'KIT': [0.066225165562913912, 0.25165562913907286, 0.072847682119205295, 0.072847682119205295, 0.066225165562913912, 0.066225165562913912, 0.27152317880794702, 0.066225165562913912, 0.066225165562913912],\n","    #      'BRAF': [0.066666666666666666, 0.17999999999999999, 0.073333333333333334, 0.073333333333333334, 0.093333333333333338, 0.080000000000000002, 0.29999999999999999, 0.066666666666666666, 0.066666666666666666],\n","    #      ...\n","    #     }\n","    gv_dict = get_gv_fea_dict(alpha, feature, df)\n","    # value_count is similar in get_gv_fea_dict\n","    value_count = train_df[feature].value_counts()\n","\n","    # gv_fea: Gene_variation feature, it will contain the feature for each feature value in the data\n","    gv_fea = []\n","    # for every feature values in the given data frame we will check if it is there in the train data then we will add the feature to gv_fea\n","    # if not we will add [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9] to gv_fea\n","    for index, row in df.iterrows():\n","        if row[feature] in dict(value_count).keys():\n","            gv_fea.append(gv_dict[row[feature]])\n","        else:\n","            gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n","#             gv_fea.append([-1,-1,-1,-1,-1,-1,-1,-1,-1])\n","    return gv_fea\n","\n","\n","# In[56]:\n","\n","\n","unique_genes = train_df['Gene'].value_counts()\n","print('Number of Unique Genes :', unique_genes.shape[0])\n","# the top 10 genes that occured most\n","print(unique_genes.head(10))\n","\n","\n","# In[57]:\n","\n","\n","print(\"Ans: There are\", unique_genes.shape[0] ,\"different categories of genes in the train data, and they are distibuted as follows\",)\n","\n","\n","# In[58]:\n","\n","\n","s = sum(unique_genes.values);\n","h = unique_genes.values/s;\n","plt.plot(h, label=\"Histrogram of Genes\")\n","plt.xlabel('Index of a Gene')\n","plt.ylabel('Number of Occurances')\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","\n","# In[59]:\n","\n","\n","c = np.cumsum(h)\n","plt.plot(c,label='Cumulative distribution of Genes')\n","plt.grid()\n","plt.legend()\n","plt.show()\n","\n","\n","# In[60]:\n","\n","\n","#response-coding of the Gene feature\n","# alpha is used for laplace smoothing\n","alpha = 1\n","# train gene feature\n","train_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", train_df))\n","# test gene feature\n","test_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", test_df))\n","# cross validation gene feature\n","cv_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", cv_df))\n","\n","\n","# In[61]:\n","\n","\n","print(\"train_gene_feature_responseCoding is converted feature using respone coding method. The shape of gene feature:\", train_gene_feature_responseCoding.shape)\n","\n","\n","# In[62]:\n","\n","\n","# one-hot encoding of Gene feature.\n","gene_vectorizer = CountVectorizer()\n","train_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\n","test_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\n","cv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])\n","\n","\n","# In[63]:\n","\n","\n","train_df['Gene'].head()\n","gene_vectorizer.get_feature_names()\n","print(\"train_gene_feature_onehotCoding is converted feature using one-hot encoding method. The shape of gene feature:\", train_gene_feature_onehotCoding.shape)\n","\n","\n","# In[64]:\n","\n","\n","unique_variations = train_df['Variation'].value_counts()\n","print('Number of Unique Variations :', unique_variations.shape[0])\n","# the top 10 variations that occured most\n","print(unique_variations.head(10))\n","\n","\n","# In[65]:\n","\n","\n","print(\"Ans: There are\", unique_variations.shape[0] ,\"different categories of variations in the train data, and they are distibuted as follows\",)\n","\n","\n","# In[66]:\n","\n","\n","s = sum(unique_variations.values);\n","h = unique_variations.values/s;\n","plt.plot(h, label=\"Histrogram of Variations\")\n","plt.xlabel('Index of a Variation')\n","plt.ylabel('Number of Occurances')\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","\n","# In[67]:\n","\n","\n","c = np.cumsum(h)\n","print(c)\n","plt.plot(c,label='Cumulative distribution of Variations')\n","plt.grid()\n","plt.legend()\n","plt.show()\n","\n","\n","# In[68]:\n","\n","\n","# alpha is used for laplace smoothing\n","alpha = 1\n","# train gene feature\n","train_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", train_df))\n","# test gene feature\n","test_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", test_df))\n","# cross validation gene feature\n","cv_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", cv_df))\n","\n","\n","# In[69]:\n","\n","\n","print(\"train_variation_feature_responseCoding is a converted feature using the response coding method. The shape of Variation feature:\", train_variation_feature_responseCoding.shape)\n","\n","\n","# In[70]:\n","\n","\n","# one-hot encoding of variation feature.\n","variation_vectorizer = CountVectorizer()\n","train_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\n","test_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\n","cv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])\n","\n","\n","# In[71]:\n","\n","\n","print(\"train_variation_feature_onehotEncoded is converted feature using the onne-hot encoding method. The shape of Variation feature:\", train_variation_feature_onehotCoding.shape)\n","\n","\n","# In[78]:\n","\n","\n","from sklearn.calibration import CalibratedClassifierCV\n","\n","alpha = [10 ** x for x in range(-5, 1)]\n","cv_log_error_array=[]\n","for i in alpha:\n","    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n","    clf.fit(train_variation_feature_onehotCoding[:,1000], y_train)\n","\n","    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","    sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n","    predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n","\n","    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n","    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n","\n","fig, ax = plt.subplots()\n","ax.plot(alpha, cv_log_error_array,c='g')\n","for i, txt in enumerate(np.round(cv_log_error_array,3)):\n","    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n","plt.grid()\n","plt.title(\"Cross Validation Error for each alpha\")\n","plt.xlabel(\"Alpha i's\")\n","plt.ylabel(\"Error measure\")\n","plt.show()\n","\n","\n","# In[79]:\n","\n","\n","print(\"Q12. How many data points are covered by total \", unique_variations.shape[0], \" genes in test and cross validation data sets?\")\n","test_coverage=test_df[test_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\n","cv_coverage=cv_df[cv_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\n","print('Ans\\n1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage/test_df.shape[0])*100)\n","print('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage/cv_df.shape[0])*100)\n","\n","\n","# In[72]:\n","\n","\n","def extract_dictionary_paddle(cls_text):\n","    dictionary = defaultdict(int)\n","    for index, row in cls_text.iterrows():\n","        for word in row['TEXT'].split():\n","            dictionary[word] +=1\n","    return dictionary\n","\n","\n","# In[80]:\n","\n","\n","# cls_text is a data frame\n","# for every row in data fram consider the 'TEXT'\n","# split the words by space\n","# make a dict with those words\n","# increment its count whenever we see that word\n","\n","def extract_dictionary_paddle(cls_text):\n","    dictionary = defaultdict(int)\n","    for index, row in cls_text.iterrows():\n","        for word in row['TEXT'].split():\n","            dictionary[word] +=1\n","    return dictionary\n","\n","\n","# In[81]:\n","\n","\n","import math\n","#https://stackoverflow.com/a/1602964\n","def get_text_responsecoding(df):\n","    text_feature_responseCoding = np.zeros((df.shape[0],9))\n","    for i in range(0,9):\n","        row_index = 0\n","        for index, row in df.iterrows():\n","            sum_prob = 0\n","            for word in row['TEXT'].split():\n","                sum_prob += math.log(((dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n","            text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))\n","            row_index += 1\n","    return text_feature_responseCoding\n","\n","\n","# In[82]:\n","\n","\n","# building a CountVectorizer with all the words that occured minimum 3 times in train data\n","text_vectorizer = CountVectorizer(min_df=3)\n","train_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\n","# getting all the feature names (words)\n","train_text_features= text_vectorizer.get_feature_names()\n","\n","# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\n","train_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n","\n","# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\n","text_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n","\n","\n","print(\"Total number of unique words in train data :\", len(train_text_features))\n","\n","\n","# In[85]:\n","\n","\n","from collections import defaultdict\n","dict_list = []\n","# dict_list =[] contains 9 dictoinaries each corresponds to a class\n","for i in range(1,10):\n","    cls_text = train_df[train_df['Class']==i]\n","    # build a word dict based on the words in that class\n","    dict_list.append(extract_dictionary_paddle(cls_text))\n","    # append it to dict_list\n","\n","# dict_list[i] is build on i'th  class text data\n","# total_dict is buid on whole training text data\n","total_dict = extract_dictionary_paddle(train_df)\n","\n","\n","confuse_array = []\n","for i in train_text_features:\n","    ratios = []\n","    max_val = -1\n","    for j in range(0,9):\n","        ratios.append((dict_list[j][i]+10 )/(total_dict[i]+90))\n","    confuse_array.append(ratios)\n","confuse_array = np.array(confuse_array)\n","\n","\n","# In[86]:\n","\n","\n","#response coding of text features\n","train_text_feature_responseCoding  = get_text_responsecoding(train_df)\n","test_text_feature_responseCoding  = get_text_responsecoding(test_df)\n","cv_text_feature_responseCoding  = get_text_responsecoding(cv_df)\n","\n","\n","# In[87]:\n","\n","\n","# https://stackoverflow.com/a/16202486\n","# we convert each row values such that they sum to 1\n","train_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\n","test_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\n","cv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T\n","\n","\n","# In[88]:\n","\n","\n","# don't forget to normalize every feature\n","train_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n","\n","# we use the same vectorizer that was trained on train data\n","test_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\n","# don't forget to normalize every feature\n","test_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n","\n","# we use the same vectorizer that was trained on train data\n","cv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n","# don't forget to normalize every feature\n","cv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)\n","\n","\n","# In[89]:\n","\n","\n","#https://stackoverflow.com/a/2258273/4084039\n","sorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\n","sorted_text_occur = np.array(list(sorted_text_fea_dict.values()))\n","\n","\n","# In[90]:\n","\n","\n","\n","\n","\n","# In[91]:\n","\n","\n","model_scores_table={'model_name':[],'train_score':[],'cv_score':[],'test_score':[],'misclassified_points':[]}\n","#model_scores_table['model_name'].append('llr')\n","model_scores_table\n","\n","\n","# In[92]:\n","\n","\n","#Data preparation for ML models.\n","\n","#Misc. functionns for ML models\n","\n","\n","def predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n","    clf.fit(train_x, train_y)\n","    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","    sig_clf.fit(train_x, train_y)\n","    pred_y = sig_clf.predict(test_x)\n","\n","    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n","    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n","    # calculating the number of data points that are misclassified\n","    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n","    plot_confusion_matrix(test_y, pred_y)\n","\n","\n","# In[93]:\n","\n","\n","def report_log_loss(train_x, train_y, test_x, test_y,  clf):\n","    clf.fit(train_x, train_y)\n","    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","    sig_clf.fit(train_x, train_y)\n","    sig_clf_probs = sig_clf.predict_proba(test_x)\n","    return log_loss(test_y, sig_clf_probs, eps=1e-15)\n","\n","\n","# In[94]:\n","\n","\n","# this function will be used just for naive bayes\n","# for the given indices, we will print the name of the features\n","# and we will check whether the feature present in the test point text or not\n","def get_impfeature_names(indices, text, gene, var, no_features):\n","    gene_count_vec = TfidfVectorizer()\n","    var_count_vec = TfidfVectorizer()\n","    text_count_vec = TfidfVectorizer(min_df=3,max_features=1000)\n","\n","    gene_vec = gene_count_vec.fit(train_df['Gene'])\n","    var_vec  = var_count_vec.fit(train_df['Variation'])\n","    text_vec = text_count_vec.fit(train_df['TEXT'])\n","\n","    fea1_len = len(gene_vec.get_feature_names())\n","    fea2_len = len(var_count_vec.get_feature_names())\n","\n","    word_present = 0\n","    for i,v in enumerate(indices):\n","        if (v < fea1_len):\n","            word = gene_vec.get_feature_names()[v]\n","            yes_no = True if word == gene else False\n","            if yes_no:\n","                word_present += 1\n","                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n","        elif (v < fea1_len+fea2_len):\n","            word = var_vec.get_feature_names()[v-(fea1_len)]\n","            yes_no = True if word == var else False\n","            if yes_no:\n","                word_present += 1\n","                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n","        else:\n","            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n","            yes_no = True if word in text.split() else False\n","            if yes_no:\n","                word_present += 1\n","                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n","\n","    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")\n","\n","\n","# In[107]:\n","\n","\n","# merging gene, variance and text features\n","\n","# building train, test and cross validation data sets\n","# a = [[1, 2],\n","#      [3, 4]]\n","# b = [[4, 5],\n","#      [6, 7]]\n","# hstack(a, b) = [[1, 2, 4, 5],\n","#                [ 3, 4, 6, 7]]\n","from scipy.sparse import hstack\n","from scipy import sparse\n","\n","train_gene_var_onehotCoding =hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n","test_gene_var_onehotCoding =hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n","cv_gene_var_onehotCoding =hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n","\n","train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\n","train_y = np.array(list(train_df['Class']))\n","\n","test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\n","test_y = np.array(list(test_df['Class']))\n","\n","cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\n","cv_y = np.array(list(cv_df['Class']))\n","\n","\n","# In[108]:\n","\n","\n","print(\"One hot encoding features :\")\n","print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n","print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n","print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)\n","\n","\n","# In[109]:\n","\n","\n","train_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\n","test_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\n","cv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n","\n","train_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\n","test_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\n","cv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n","\n","\n","# In[110]:\n","\n","\n","print(\" Response encoding features :\")\n","print(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\n","print(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\n","print(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)\n","\n","\n","# In[116]:\n","\n","\n","#knn classificaton hyper paramter tuning\n","\n","\n","alpha = [5, 11, 15, 21, 31, 41, 51, 99]\n","cv_log_error_array = []\n","for i in alpha:\n","    print(\"for alpha =\", i)\n","    clf = KNeighborsClassifier(n_neighbors=i)\n","    clf.fit(train_x_responseCoding, train_y)\n","    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","    sig_clf.fit(train_x_responseCoding, train_y)\n","    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n","    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n","    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n","    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n","\n","fig, ax = plt.subplots()\n","ax.plot(alpha, cv_log_error_array,c='g')\n","for i, txt in enumerate(np.round(cv_log_error_array,3)):\n","    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n","plt.grid()\n","plt.title(\"Cross Validation Error for each alpha\")\n","plt.xlabel(\"Alpha i's\")\n","plt.ylabel(\"Error measure\")\n","plt.show()\n","\n","\n","best_alpha = np.argmin(cv_log_error_array)\n","clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n","clf.fit(train_x_responseCoding, train_y)\n","sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","sig_clf.fit(train_x_responseCoding, train_y)\n","\n","predict_y = sig_clf.predict_proba(train_x_responseCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n","predict_y = sig_clf.predict_proba(cv_x_responseCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n","predict_y = sig_clf.predict_proba(test_x_responseCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","\n","\n","# In[115]:\n","\n","\n","#sample query point\n","from collections import Counter\n","clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n","clf.fit(train_x_responseCoding, train_y)\n","sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","sig_clf.fit(train_x_responseCoding, train_y)\n","\n","test_point_index = 1\n","predicted_cls = sig_clf.predict(test_x_responseCoding[0].reshape(1,-1))\n","print(\"Predicted Class :\", predicted_cls[0])\n","print(\"Actual Class :\", test_y[test_point_index])\n","neighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\n","print(\"The \",alpha[best_alpha],\" nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\n","print(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))\n","\n","\n","# In[117]:\n","\n","\n","clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n","clf.fit(train_x_responseCoding, train_y)\n","sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","sig_clf.fit(train_x_responseCoding, train_y)\n","\n","test_point_index = 100\n","\n","predicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\n","print(\"Predicted Class :\", predicted_cls[0])\n","print(\"Actual Class :\", test_y[test_point_index])\n","neighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\n","print(\"the k value for knn is\",alpha[best_alpha],\"and the nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\n","print(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))\n","\n","\n","# In[118]:\n","\n","\n","#svm\n","\n","\n","alpha = [10 ** x for x in range(-5, 3)]\n","cv_log_error_array = []\n","for i in alpha:\n","    print(\"for C =\", i)\n","# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n","    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n","    clf.fit(train_x_onehotCoding, train_y)\n","    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","    sig_clf.fit(train_x_onehotCoding, train_y)\n","    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n","    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n","    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n","fig, ax = plt.subplots()\n","ax.plot(alpha, cv_log_error_array,c='g')\n","for i, txt in enumerate(np.round(cv_log_error_array,3)):\n","    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n","plt.grid()\n","plt.title(\"Cross Validation Error for each alpha\")\n","plt.xlabel(\"Alpha i's\")\n","plt.ylabel(\"Error measure\")\n","plt.show()\n","\n","\n","best_alpha = np.argmin(cv_log_error_array)\n","# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n","clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n","clf.fit(train_x_onehotCoding, train_y)\n","sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","sig_clf.fit(train_x_onehotCoding, train_y)\n","\n","predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n","predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n","predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","\n","\n","# In[119]:\n","\n","\n","#logistic regression\n","\n","alpha = [10 ** x for x in range(-6, 3)]\n","cv_log_error_array = []\n","for i in alpha:\n","    print(\"for alpha =\", i)\n","    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n","    clf.fit(train_x_onehotCoding, train_y)\n","    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","    sig_clf.fit(train_x_onehotCoding, train_y)\n","    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n","    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n","    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n","    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n","fig, ax = plt.subplots()\n","ax.plot(alpha, cv_log_error_array,c='g')\n","for i, txt in enumerate(np.round(cv_log_error_array,3)):\n","    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n","plt.grid()\n","plt.title(\"Cross Validation Error for each alpha\")\n","plt.xlabel(\"Alpha i's\")\n","plt.ylabel(\"Error measure\")\n","plt.show()\n","best_alpha = np.argmin(cv_log_error_array)\n","clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n","clf.fit(train_x_onehotCoding, train_y)\n","sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n","sig_clf.fit(train_x_onehotCoding, train_y)\n","\n","predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n","predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n","predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n","print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"]}]}